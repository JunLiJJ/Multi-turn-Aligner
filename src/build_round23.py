import os, json, time, random
from pathlib import Path
from typing import List, Dict
from tenacity import retry, wait_exponential_jitter, stop_after_attempt
from tqdm import tqdm
from openai import OpenAI
from dotenv import load_dotenv

import google.generativeai as genai
from prompts import (
    FOLLOWUP_SYSTEM, FOLLOWUP_USER_TMPL,
    LLAMA_SYSTEM,
    CORRECTION_SYSTEM, CORRECTION_USER_TMPL
)

# ====== åŠ è½½ .env æ–‡ä»¶ ======
load_dotenv()

# ====== é…ç½®å‚æ•° ======
NUM_ROUNDS = 2  # ğŸ¯ è¦ç”Ÿæˆå‡ è½®å¯¹è¯ï¼ˆä»ç¬¬2è½®å¼€å§‹ç®—ï¼‰
INP = Path("data/round1/round1_history_seed.jsonl")

# ====== Gemini åˆå§‹åŒ– (ç”¨äºç”Ÿæˆ followup å’Œ correction) ======
GEMINI_KEY = os.environ.get("GEMINI_API_KEY")
if not GEMINI_KEY:
    raise RuntimeError("âŒ Please set GEMINI_API_KEY in .env file")
genai.configure(api_key=GEMINI_KEY)
GEMINI_MODEL = "gemini-2.5-flash-lite"

# ====== Together AI åˆå§‹åŒ– (ç”¨äºç”Ÿæˆ answer) ======
TOGETHER_KEY = os.environ.get("TOGETHER_API_KEY")
if not TOGETHER_KEY:
    raise RuntimeError("âŒ Please set TOGETHER_API_KEY in .env file")
together_client = OpenAI(
    api_key=TOGETHER_KEY,
    base_url="https://api.together.xyz/v1"
)
LLAMA_MODEL = "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"


# è¿™é‡ŒæŠŠå†å²ä¿ç•™ questionã€answer å’Œ correction
def _rounds_to_str(rounds: List[dict]) -> str:
    """å°† rounds åˆ—è¡¨è½¬ä¸ºå¯¹è¯å†å²å­—ç¬¦ä¸²ï¼ˆåŒ…å« question, answer, correctionï¼‰"""
    lines = []
    for r in rounds:
        q = (r.get("question") or "").strip()
        a = (r.get("answer") or "").strip()
        c = (r.get("correction") or "").strip()
        if q:
            lines.append(f"User: {q}")
        if a:
            lines.append(f"Assistant: {a}")
        if c:
            lines.append(f"Assistant: {c}")
    return "\n".join(lines[-16:])  # ä¿ç•™æœ€è¿‘16æ¡æ¶ˆæ¯ï¼ˆçº¦8è½®å¯¹è¯ï¼‰



def _rounds_to_messages(rounds: List[dict]) -> List[Dict]:
    """å°† rounds è½¬ä¸º OpenAI æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å« question, answer, correctionï¼‰"""
    messages = []
    for r in rounds:
        q = (r.get("question") or "").strip()
        a = (r.get("answer") or "").strip()
        c = (r.get("correction") or "").strip()
        if q:
            messages.append({"role": "user", "content": q})
        if a:
            messages.append({"role": "assistant", "content": a})
        if c:
            messages.append({"role": "assistant", "content": c})
    return messages[-16:]  # ä¿ç•™æœ€è¿‘16æ¡æ¶ˆæ¯



@retry(wait=wait_exponential_jitter(initial=1, max=12), stop=stop_after_attempt(6))
def generate_followup(history_str: str, q: str, a: str, c: str) -> str:
    """ç”¨ Gemini ç”Ÿæˆè¿½é—®"""
    user_msg = FOLLOWUP_USER_TMPL.format(history_str=history_str, q=q, a=a, c=c)
    
    model = genai.GenerativeModel(GEMINI_MODEL, system_instruction=FOLLOWUP_SYSTEM)
    resp = model.generate_content(
        contents=[{"role": "user", "parts": [user_msg]}],
        generation_config={
            "temperature": 0.7,
            "top_p": 0.9,
            "max_output_tokens": 150,
            "response_mime_type": "application/json",
        },
        safety_settings=None,
    )

    txt = (resp.text or "").strip()
    if txt.startswith("```"):
        txt = txt.strip("`")
        if "\n" in txt:
            txt = txt.split("\n", 1)[1].rsplit("\n", 1)[0].strip()

    try:
        data = json.loads(txt)
    except json.JSONDecodeError:
        if txt.lower().startswith("followup"):
            data = {"followup": txt.split(":", 1)[-1].strip()}
        else:
            raise

    followup = (data.get("followup") or "").strip()
    if not followup:
        raise ValueError("Empty followup")
    return followup

@retry(wait=wait_exponential_jitter(initial=1, max=12), stop=stop_after_attempt(6))
def generate_answer(history_messages: List[Dict], question: str) -> str:
    """ç”¨ Llama åŸºäº history ç”Ÿæˆå›ç­”"""
    # ä½¿ç”¨ prompts.py ä¸­å®šä¹‰çš„ç³»ç»Ÿæç¤º
    system_msg = {"role": "system", "content": LLAMA_SYSTEM}
    messages = [system_msg] + history_messages + [{"role": "user", "content": question}]
    
    resp = together_client.chat.completions.create(
        model=LLAMA_MODEL,
        messages=messages,
        temperature=0.7,
        max_tokens=300,  # çº¦500å­—ç¬¦ï¼ˆ1å­—ç¬¦â‰ˆ1.5-2tokensï¼‰
    )
    
    answer = resp.choices[0].message.content.strip()
    if not answer:
        raise ValueError("Empty answer")
    return answer

@retry(wait=wait_exponential_jitter(initial=1, max=12), stop=stop_after_attempt(6))
def generate_correction(history_str: str, question: str, answer: str) -> str:
    """ç”¨ Gemini ç”Ÿæˆä¿®æ­£åçš„å›ç­”"""
    user_msg = CORRECTION_USER_TMPL.format(
        history_str=history_str,
        question=question,
        answer=answer
    )
    
    model = genai.GenerativeModel(GEMINI_MODEL, system_instruction=CORRECTION_SYSTEM)
    resp = model.generate_content(
        contents=[{"role": "user", "parts": [user_msg]}],
        generation_config={
            "temperature": 0.6,
            "top_p": 0.9,
            "max_output_tokens": 300,  # å¢åŠ åˆ°300ï¼Œå…è®¸è¯¦ç»†çš„ä¿®æ­£å›ç­”ï¼ˆ2-4å¥è¯ï¼‰
            "response_mime_type": "application/json",
        },
        safety_settings=None,
    )

    txt = (resp.text or "").strip()
    if txt.startswith("```"):
        txt = txt.strip("`")
        if "\n" in txt:
            txt = txt.split("\n", 1)[1].rsplit("\n", 1)[0].strip()

    try:
        data = json.loads(txt)
    except json.JSONDecodeError:
        # å…œåº•ï¼šç›´æ¥è¿”å›åŸç­”æ¡ˆ
        return answer

    correction = (data.get("correction") or "").strip()
    if not correction:
        return answer  # å…œåº•
    return correction

def main():
    """è¿­ä»£ç”Ÿæˆå¤šè½®å¯¹è¯"""
    # è¯»å–åˆå§‹æ•°æ®
    with INP.open("r", encoding="utf-8") as f:
        initial_data = [json.loads(line) for line in f]
    
    print(f"ğŸ“š Loaded {len(initial_data)} samples from {INP}")
    print(f"ğŸ¯ Will generate {NUM_ROUNDS} additional rounds\n")
    
    # è¿­ä»£ç”Ÿæˆæ¯ä¸€è½®
    for round_num in range(2, 2 + NUM_ROUNDS):
        print(f"\n{'='*60}")
        print(f"ğŸ”„ Generating Round {round_num}")
        print(f"{'='*60}\n")
        
        # è¾“å‡ºè·¯å¾„
        out_dir = Path(f"data/round{round_num}")
        out_dir.mkdir(parents=True, exist_ok=True)
        out_history = out_dir / f"round{round_num}_history.jsonl"
        out_single = out_dir / f"round{round_num}_single_turn.jsonl"
        
        new_data = []
        
        with out_history.open("w", encoding="utf-8") as f_hist, \
             out_single.open("w", encoding="utf-8") as f_single:
            
            for ex in tqdm(initial_data, desc=f"Round {round_num}"):
                rid = ex.get("id", "")
                rounds = ex.get("rounds", []) or []
                
                if not rounds:
                    continue
                
                # è·å–æœ€åä¸€è½®çš„ q, a, c
                last_round = rounds[-1]
                last_q = last_round.get("question", "") or ""
                last_a = last_round.get("answer", "") or ""
                last_c = last_round.get("correction", "") or ""
                
                # æ„å»ºå†å²ä¸Šä¸‹æ–‡
                hist_str = _rounds_to_str(rounds[:-1]) if len(rounds) > 1 else ""
                hist_msgs = _rounds_to_messages(rounds)
                
                try:
                    # Step 1: Gemini ç”Ÿæˆè¿½é—®
                    followup_q = generate_followup(hist_str, last_q, last_a, last_c)
                    
                    # Step 2: Llama ç”Ÿæˆå›ç­”
                    followup_a = generate_answer(hist_msgs, followup_q)
                    
                    # Step 3: Gemini ç”Ÿæˆä¿®æ­£
                    full_hist_str = _rounds_to_str(rounds)  # åŒ…å«æ‰€æœ‰å†å²
                    followup_c = generate_correction(full_hist_str, followup_q, followup_a)
                    
                except Exception as e:
                    print(f"âš ï¸  Error for {rid}: {type(e).__name__}: {str(e)[:100]}")
                    print(f"   Using fallback responses...")
                    followup_q = "Could you provide more details?"
                    followup_a = "I need more context to answer properly."
                    followup_c = "Let me know what specific aspect you'd like me to clarify."
                
                # æ„å»ºæ–°ä¸€è½®
                new_round = {
                    "question": followup_q,
                    "answer": followup_a,
                    "correction": followup_c
                }
                
                # è¿½åŠ åˆ° rounds
                updated_rounds = rounds + [new_round]
                
                # ä¿å­˜å®Œæ•´ history
                f_hist.write(json.dumps({
                    "id": rid,
                    "rounds": updated_rounds
                }, ensure_ascii=False) + "\n")
                
                # ä¿å­˜å•è½®æ•°æ®
                f_single.write(json.dumps({
                    "id": rid,
                    **new_round
                }, ensure_ascii=False) + "\n")
                
                # æ›´æ–°æ•°æ®ä¾›ä¸‹ä¸€è½®ä½¿ç”¨
                new_data.append({"id": rid, "rounds": updated_rounds})
                
                # æ¸©å’Œé€Ÿç‡
                time.sleep(random.uniform(0.1, 0.3))
        
        print(f"\nâœ… Round {round_num} completed!")
        print(f"   ğŸ“„ History: {out_history}")
        print(f"   ğŸ“„ Single turn: {out_single}")
        
        # æ›´æ–° initial_data ä¾›ä¸‹ä¸€è½®è¿­ä»£
        initial_data = new_data
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ All {NUM_ROUNDS} rounds generated successfully!")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
